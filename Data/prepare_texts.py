import fitz
import re
import os

import nltk
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')

from nltk.stem import SnowballStemmer
from nltk.corpus import stopwords

from gensim import corpora


def save_to_txt():
    
    files = os.listdir('Data/clean')
    os.makedirs('Data/filtered', exist_ok=True)

    for file in files:
        with fitz.open(f'Data/clean/{file}') as doc: 
            
            file_text = ''
            for page_num in range(len(doc)):
                text = doc.load_page(page_num).get_text()

                # Fixing incorrect symbols
                text = re.sub(r'ï¬', 'fi', text)
                text = re.sub(r'ï¿½', '', text)
                text = re.sub(r'ï¬€', 'ff', text)
                text = re.sub(r'ï¬‚', 'fl', text)

                # Removing part that goes before "Abstract"
                text = re.sub(r'(.|\n)*abstract', ' ', text, flags=re.IGNORECASE)
                

                # Removing "Table" and "Figure" description
                text = re.sub(r'^table(.|\n)*?\.\n', '', text, flags=re.IGNORECASE)
                text = re.sub(r'^(Figure|Fig)(.|\n)*?\.\n', '', text)

                # Fixing word-hyphenations
                text = re.sub(r'-\n(?=.)', '', text) 
                
                # Fixing sentence-hyphenation
                text = re.sub(r'(?<=[^\.])\n(?=\S)', ' ', text) 
                
                # Removing brackets and its content
                text = re.sub(r'\s*\((.|\n)*?\)\s*', ' ', text) 
                text = re.sub(r'\s*\[(.|\n)*?\]\s*', ' ', text)
                text = re.sub(r'\s*\{(.|\n)*?\}\s*', ' ', text)

                # Removing math expressions
                text = re.sub(r'(\s\w?[xyXY]\w?[\s,]|[a-zA-Z]â€²| [a-zA-Z]([0-9]|,) |\S*[0-9][^\s.]*|:(.|\n)*?(?=\.)|\S*=\S*|\S*\|+\S*|\S*Ïƒ\S*|\S*\(+\S*|\S*\)+\S*|\S*Î»\S*|\S*Ï•\S*|\S*âˆ†\S*|\S*âˆ‡\S*|\S*âˆ€\S*|\S*\+\S*|\S*Â±\S*|\S*Ïµ\S*|\S*>\S*|\S*â†\S*|\S*\\\S*|\S*\*\S*|\S*%\S*| âˆ’ |\S*Ã—\S*|\S*Î·\S*|\S*Âµ\S*|\S*log\S*|\S*âˆˆ\S*|\S*Ï\S*|\S*Î¸\S*|\S*Î±\S*|\S*Î˜\S*|\S*Î¦\S*|\S*âŠ™\S*|\S*lim\S*|â‹„|â€ |â€¢|\S*ğ´\S*|\S*Ëœ\S*|\S*ğœ‹\S*|\S*Ã˜\S*|âŒŠ|âŒ‹|â‰¤|â€²|\S*Î´\S*|\S*ğ‘¤\S*|\S*ğ‘\S*|\S*ğœ•\S*)|ğ€|\S*Ï‰\S*|âˆ|ln', ' ', text) 
                
                # Removing separated variables with length 1
                text = re.sub(r'(?<= )\w(?= )', ' ', text)

                # Removing duplecated spaces
                text = re.sub(r' +', ' ', text)

                if re.search('references', text, re.IGNORECASE):
                    text = re.sub(r'references(.|\n)*', '', text, flags=re.IGNORECASE)
                    file_text += text
                    break
                else:
                    file_text += text

            file_lines = file_text.split('\n')
            file_lines = [line for line in file_text.split('\n') if len(line.split(' ')) > 4]
            file_text = ' '.join(file_lines)

            file = open(f'Data/filtered/{file[:file.rfind(".")]}.txt', 'w', encoding='utf-8')
            file.write(file_text)
            file.close()


def remove_stop_words(text):
    
    stop = set(stopwords.words('english'))
    stop.add('learn')
    stop.add('use')
    stop.add('task')
    stop.add('train')

    sentences = text.split('.')
    new_sentences = []
    for sentence in sentences:
        new_sentences.append(' '.join([word.lower() for word in sentence.split() if word.lower() not in stop]))

    return '.'.join(new_sentences)


def stemming(text):

    snowball_stemmer = SnowballStemmer('english')

    sentences = text.split('.')
    new_sentences = []
    for sentence in sentences:
        word_tokens = nltk.word_tokenize(sentence)
        stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]
        new_sentences.append(' '.join(stemmed_word))

    return '.'.join(new_sentences)

def get_docs():

    if not os.path.exists('Data/filtered') or len(os.listdir('Data/filtered')) == 0:
        save_to_txt()

    docs = []
    doc_list = os.listdir('Data/filtered')
    for doc_name in doc_list:
        with open(f'Data/filtered/{doc_name}', 'r', encoding='utf_8') as doc:
            text = doc.read()
            text = remove_stop_words(text)
            text = stemming(text)

            text = re.sub(r',', '', text)

            sentences = [sentence for sentence in text.split('.') if len(sentence.split(' ')) > 2]
            final_sentences = []
            while len(sentences) > 0:
                final_sentences.append(' '.join(sentences[:1]))
                del sentences[:1]
            docs.extend(final_sentences)

    return docs

def retrieve_data():

    docs = get_docs()

    texts = [[word for word in doc.split()] for doc in docs]
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]

    return texts, dictionary, corpus