 Few-shot learning refers to understanding new concepts from only few examples. We propose an information retrieval-inspired approach for this problem that is motivated the increased importance of maximally leveraging all the available information in this low-data regime. We define training objective that aims to extract as much information as possible from each training batch effectively optimizing over all relative orderings of the batch points simultaneously. In particular, we view each batch point as ‘query’ that ranks the remaining ones based on its predicted relevance to them and we define model within the framework of structured prediction to optimize mean Average Precision over these rankings. Our method achieves impressive results on the standard few-shot classification benchmarks while is also capable of few-shot retrieval.  Introduction Recently, the problem of learning new concepts from only few labelled examples, referred to as few-shot learning, has received considerable attention . More concretely, K-shot N-way classification is the task of classifying data point into one of classes, when only examples of each class are available to inform this decision. This is challenging setting that necessitates different approaches from the ones commonly employed when the labelled data of each new concept is abundant. Indeed, many recent success stories of machine learning methods rely on large datasets and suffer from overfitting in the face of insufficient data. It is however not realistic nor preferred to always expect many examples for learning new class or concept, rendering few-shot learning an important problem to address. We propose model for this problem that aims to extract as much information as possible from each training batch, capability that is of increased importance when the available data for learning each class is scarce. Towards this goal, we formulate few-shot learning in information retrieval terms . We are then faced with the choice of ranking loss function and computational framework for optimization. We choose to work within the framework of structured prediction and we optimize mean Average Precision using standard Structural SVM , as well as Direct Loss Minimization approach. We argue that the objective of mAP is especially suited for the low-data regime of interest since it allows us to fully exploit each batch simultaneously optimizing over all relative orderings of the batch points. Figure provides an illustration of this training objective. Our contribution is therefore to adopt an information retrieval perspective on the problem of few-shot learning; we posit that model is prepared for the sparse-labels setting being trained in manner that fully exploits the information in each batch. We also introduce new form of few-shot learning task, ‘few-shot retrieval’, where given ‘query’ image and pool of candidates all coming from previously-unseen classes, the task is to ‘retrieve’ all relevant candidates for the query. We achieve competitive with the state-of-the-art results on the standard few-shot classification benchmarks and show superiority over strong baseline in the proposed few-shot retrieval problem.  Related Work Our approach to few-shot learning heavily relies on learning an informative similarity metric, goal that has been extensively studied in the area of metric learning. This can be thought of as learning mapping of objects into space where their relative positions are indicative of their similarity relationships. We refer the reader to survey of metric learning and merely touch upon few representative methods here. Neighborhood Component Analysis learns metric aiming at high performance in nearest neirhbour classification. Large Margin Nearest Neighbor refers to another approach for nearest neighbor classification which constructs triplets and employs contrastive loss to move the ‘anchor’ of each triplet closer to the similarly-labelled point and farther from the dissimilar one at least predefined margin. More recently, various methods have emerged that harness the power of neural networks for metric learning. These methods vary in terms of loss functions but have in common mechanism for the parallel and identically-parameterized embedding of the points that will inform the loss function. Siamese and triplet networks are commonly-used variants of this family that operate on pairs and triplets, respectively. Example applications include signature verification and face verification . NCA and LMNN have also been extended to their deep variants and , respectively. These methods often employ hard-negative mining strategies for selecting informative constraints for training . drawback of siamese and triplet networks is that they are local, in the sense that their loss function concerns pairs or triplets of training examples, guiding the learning process to optimize the desired relative positions of only two or three examples at time. The myopia of these local methods introduces drawbacks that are reﬂected in their embedding spaces. propose method to address this using higher-order information. We also learn similarity metric in this work, but our approach is specifically tailored for few-shot learning. Other metric learning approaches for few-shot learning include . employs deep convolutional neural network that is trained to correctly predict pairwise similarities. Attentive Recurrent Comparators also perform pairwise comparisons but form the representation of the pair through sequence of at the two points that comprise it via recurrent neural network. We note that these pairwise approaches do not offer natural mechanism to solve K-shot N-way tasks for and focus on one-shot learning, whereas our method tackles the more general few-shot learning problem. Matching Networks aim to ‘match’ the training setup to the evaluation trials of K-shot N-way classification . Prototypical Networks also perform episodic training, and use the simple yet effective mechanism of representing each class the mean of its examples in the support set, constructing  ‘prototype’ in this way that each query example will be compared with. Our approach can be thought of as constructing all such query/support sets within each batch in order to fully exploit it. Another family of methods for few-shot learning is based on meta-learning. Some representative work in this category includes . These approaches present models that learn how to use the support set in order to update the parameters of learner model in such way that it can generalize to the query set. Meta-Learner LSTM learns an initialization for learners that can solve new tasks, whereas Model-Agnostic Meta-Learner learns an update step that learner can take to be successfully adapted to new task. Finally, presents method that uses an external memory module that can be integrated into models for remembering rarely occurring events in life-long learning setting. They also demonstrate competitive results on few-shot classification.  Background Mean Average Precision Consider batch of points . Let be the set of points that are relevant to determined in binary fashion according to class membership. Let denote the ranking based on the predicted similarity between and the remaining points in so that stores jth most similar point. Precision at in the ranking denoted is the proportion of points that are relevant to within the highest-ranked ones. The Average Precision of this ranking is then computed averaging the precisions at over all positions in that store relevant points.  where Finally, mean Average Precision calculates the mean AP across batch points.  AP Structural Support Vector Machine Structured prediction refers to family of tasks with inter-dependent structured output variables such as trees, graphs, and sequences, to name just few . Our proposed learning objective that involves producing ranking over set of candidates also falls into this category so we adopt structured prediction as our computational framework. SSVM is an efficient method for these tasks with the advantage of being tunable to custom task loss functions. More concretely, let and denote the spaces of inputs and structured outputs, respectively. Assume scoring function depending on some weights and task loss incurred when predicting when the groundtruth is yGT. The margin-rescaled SSVM optimizes an upper bound of the task loss formulated as Direct Loss Minimization proposed method that directly optimizes the task loss of interest instead of an upper bound of it. In particular, they provide perceptron-like weight update rule that they prove corresponds to the gradient of the task loss. present theorem that equips us with the corresponding weight update rule for the task loss in the case of nonlinear models, where the scoring function is parameterized neural network. Since we make use of their theorem, we include it below for completeness. Let be dataset composed of input and output pairs. Let be scoring function which depends on the input, the output and some parameters RA. Theorem . When given finite set scoring function , data distribution, as well as task-loss , then, under some mild regularity conditions, the direct loss gradient has the following form . and provide an intuitive view for each one. In the case of the positive update, ydirect can be thought of as the ‘worst’ solution since it corresponds to the output value that achieves high score while producing high task loss. In this case, the positive update encourages the model to move away from the bad solution ydirect. On the other hand, when performing the negative update, ydirect represents the ‘best’ solution . The model is hence encouraged in this case to adjust its weights towards the direction of the gradient of this best solution’s score. In nutshell, this theorem provides us with the weight update rule for the optimization of custom task loss, provided that we define scoring function and procedures for performing standard and loss-augmented inference.  Relationship between DLM and SSVM As also noted in , the positive update of direct loss minimization strongly resembles that of the margin-rescaled structural SVM which also yields loss-informed weight update rule. This gradient computation differs from that of the direct loss minimization approach only in that, while SSVM considers the score of the ground-truth , direct loss minimization considers the score of the current prediction . The computation of yhinge strongly resembles that of ydirect in the positive update. Indeed SSVM’s training procedure also encourages the model to move away from weights that produce the ‘worst’ solution yhinge.  Optimizing for Average Precision In the following section we adapt and extend method for optimizing AP . Given query point, the task is to rank points with respect to their relevance to the query, where point is relevant if it belongs to the same class as the query and irrelevant otherwise. Let and be the sets of ‘positive’ and ‘negative’ points respectively. The output ranking is represented as yij pairs where yij if is ranked higher than and yij otherwise, and yii . Define to be the collection of all such pairwise rankings. The scoring function that used is borrowed from and . devise dynamic programming algorithm to perform loss-augmented inference in this setting which we make use of but we omit for brevity.  Few-Shot Learning Optimizing mAP In this section, we present our approach for few-shot learning that optimizes mAP. We extend the work of that optimizes for AP in order to account for all possible choices of query among the batch points. This is not straightforward extension as it requires ensuring that optimizing the AP of one query’s ranking does not harm the AP of another query’s ranking. In what follows we define mathematical framework for this problem and we show that we can treat each query independently without sacrificing correctness, therefore allowing to efficiently in parallel  learn to optimize all relative orderings within each batch. We then demonstrate how we can use the frameworks of SSVM and DLM for optimization of mAP, producing two variants of our method henceforth referred to as mAP-SSVM and mAP-DLM, respectively. Setup . Each class defines the positive set Pc containing the points that belong to and the negative set containing the rest of the points. We denote ci the class label of the ith point. We represent the output rankings as collection of kj variables where kj if is ranked higher than in i’s ranking, kk and kj if is ranked higher than in i’s ranking. For convenience we combine these comparisons for each query in . Let be the embedding function, parameterized neural network and the cosine similarity of points and in the embedding space given . We consider for each query the function . Then the AP loss for the ranking induced some query is defined as . We define the mAP loss to be the average AP loss over all query points. Inference . Specifically, for query of class the computation of the kj’s, Pc can happen independently of the computation of the ’s for some other query i. We are thus able to optimize the ordering induced each query point independently of those induced the other queries. For query positive point and negative point the solution of standard inference is wkj arg maxyi and can be computed as follows wkj if otherwise Loss-augmented inference for query is defined as direct arg max and can be performed via run of the dynamic programming algorithm of . We can then combine the results of all the independent inferences to compute the overall scoring function   and   Finally, we define the ground-truth output value yGT . For any query and distinct points we set GTmn if Pci and ci, GTmn if Pci and ci and GTmn otherwise. We note that construction of our scoring function defined above, we will only have to compute kj’s where and belong to the same class ci and is point from another class. Because of this, we set the GT for each query to be an appropriately-sized matrix of ones . The overall score of the ground truth is then   Optimizing mAP via SSVM and DLM We have now defined all the necessary components to compute the gradient update as specified the General Loss Gradient Theorem of in equation or as defined the Structural SVM in equation . For clarity, Algorithm describes this process, outlining the two variants of our approach for few-shot learning, namely mAP-DLM and mAP-SSVM.  Evaluation In what follows, we describe our training setup, the few-shot learning tasks of interest, the datasets we use, and our experimental results. Through our experiments, we aim to evaluate the few-shot retrieval ability of our method and additionally to compare our model to competing approaches for few-shot classification. For this, we have updated our tables to include very recent work that is published concurrently with ours in order to provide the reader with complete view of the state-of-the-art on few-shot learning. Finally, we also aim to investigate experimentally our model’s aptness for learning from little data via its training objective that is designed to fully exploit each training batch. Controlling the inﬂuence of loss-augmented inference on the loss gradient We found empirically that for the positive update of mAP-DLM and for mAP-SSVM, it is beneficial to introduce hyperparamter that controls the contribution of the loss-augmented relative to that of in the case of mAP-DLM, or in the case of mAP-SSVM. The updated rules that we use in practice for training mAP-DLM and mAP-SSVM, respectively, are shown below, where is hyperparamter.  and We refer the reader to the supplementary material for more details concerning this hyperparameter. Few-shot Classification and Retrieval Tasks Each K-shot N-way classification ‘episode’ is constructed as follows . For each class, out of the images are randomly chosen to act as the ‘representatives’ of that class. The remaining images of each class are then to be classified among the classes. This poses total of classification problems. Following the standard procedure, we repeat this process times when testing on Omniglot and times for mini-ImageNet in order to compute the results reported in tables and . We also designed similar one-shot N-way retrieval task, where to form each episode we select classes at random and images per class, yielding pool of images. Each of these images acts as query and ranks all remaining images. The goal is to retrieve all relevant images before any of the irrelevant ones. We measure the performance on this task using mAP. Note that since this is new task, there are no publicly available results for the competing few-shot learning methods. Our Algorithm for K-shot N-way classification Our model classifies image into class arg maxi AP , where AP denotes the average precision of the ordering that image assigns to the pool of all KN representatives assuming that the ground truth class for image is i. This means that when computing AP , the representatives of class will have binary relevance of while the representatives of the other classes will have binary relevance of . Note that in the one-shot learning case where this amounts to classifying into the class whose representative is most similar to according to the model’s learned similarity metric. We note that the siamese model does not naturally offer procedure for exploiting all representatives of each class when making the classification decision for some reference. Therefore we omit few-shot learning results for siamese when and examine this model only in the one-shot case. Training details We use the same embedding architecture for all of our models for both Omniglot and mini-ImageNet. This architecture mimics that of and consists of identical blocks stacked upon each other. Each of these blocks consists of convolution with filters, batch normalization , ReLU activation, and max-pooling. We resize the Omniglot images to and the mini-ImageNet images to therefore producing feature vector for each Omniglot image and one for each mini-ImageNet image. We use ADAM for training all models. We refer the reader to the supplementary for more details. Omniglot The Omniglot dataset is designed for testing few-shot learning methods. This dataset consists of characters from different alphabets, with each character drawn different drawers. Following , we use characters as training classes and the remaining for evaluation while we also augment the dataset with random rotations multiples of degrees. The results for this dataset are shown in Table . Both mAP-SSVM and mAP-DLM are trained with and for mAP-DLM the positive update was used. We used and for our models and the siamese. Overall, we observe that many methods perform very similarly on few-shot classification on this dataset, ours being among the top-performing ones. Further, we perform equally well or better than the siamese network in few-shot retrieval. We’d like to emphasize that the siamese network is tough baseline to beat, as can be seen from its performance in the classification tasks where it outperforms recent few-shot learning methods. mini-ImageNet mini-ImageNet refers to subset of the dataset that was used as benchmark for testing few-shot learning approaches in . This dataset contains color images and constitutes significantly more challenging benchmark than Omniglot. In order to compare our method with the state-of-the-art on this benchmark, we adapt the splits introduced in which contain total of classes out of which are used for training, for validation and for testing. We train our models on the training set and use the validation set for monitoring performance. Table reports the performance of our method and recent competing approaches on this benchmark. As for Omniglot, the results of both versions of our method are obtained with and with the positive update in the case of mAP-DLM. We used and for our models and the siamese. We also borrow the baseline reported in for this task which corresponds to performing nearest-neighbors on top of the learned embeddings. Our method yields impressive results here, outperforming recent approaches tailored for few-shot learning either via deep-metric learning such as Matching Networks or via meta-learning such as Meta-Learner LSTM and MAML in few-shot classification. We set the new state-of-the-art for classification. Further, our models are superior than the strong baseline of the siamese network in the few-shot retrieval tasks. CUB We also experimented on the Caltech-UCSD Birds dataset , where we outperform the siamese network as well. More details can be found in the supplementary. Learning Efficiency We examine our method’s learning efficiency via comparison with siamese network. For fair comparison of these models, we create the training batches in way that enforces that they have the same amount of information available for each update . The siamese network is then trained on all possible pairs from these sampled points. Figure displays the performance of our model and the siamese on different metrics on Omniglot and mini-ImageNet. The first two rows show the performance of our two variants and the siamese in the few-shot classification and few-shot retrieval tasks, for various levels of difficulty as regulated the different values of N. The first row corresponds to Omniglot and the second to mini-ImageNet. We observe that even when both methods converge to comparable accuracy or mAP values, our method learns faster, especially when the ‘way’ of the evaluation task is larger, making the problem harder. In the third row in Figure we examine the few-shot learning performance of our model and the all-pairs siamese that were trained with but with different We note that for given larger batch size implies larger ‘shot’. For example, for results to on average examples of each class in each batch whereas results to on average . We observe that especially when the ‘shot’ is smaller, there is clear advantage in using our method over the all-pairs siamese. Therefore it indeed appears to be the case that the fewer examples we are given per class, the more we can benefit from our structured objective that simultaneously optimizes all relative orderings. Further, mAP-DLM can reach higher performance overall with smaller batch sizes than the siamese, indicating that our method’s training objective is indeed efficiently exploiting the batch examples and showing promise in learning from less data. Discussion It is interesting to compare experimentally methods that have pursued different paths in addressing the challenge of few-shot learning. In particular, the methods we compare against each other in our tables include deep metric learning approaches such as ours, the siamese network, Prototypical Networks and Matching Networks, as well as meta-learning methods such as MetaLearner LSTM and MAML . Further, has metric-learning ﬂavor but employs external memory as vehicle for remembering representations of rarely-observed classes. The experimental  results suggest that there is no clear winner category and all these directions are worth exploring further. Overall, our model performs on par with the state-of-the-art results on the classification benchmarks, while also offering the capability of few-shot retrieval where it exhibits superiority over strong baseline. Regarding the comparison between mAP-DLM and mAP-SSVM, we remark that they mostly perform similarly to each other on the benchmarks considered. We have not observed in this case significant win for directly optimizing the loss of interest, offered mAP-DLM, as opposed to minimizing an upper bound of it.  Conclusion We have presented an approach for few-shot learning that strives to fully exploit the available information of the training batches, skill that is utterly important in the low-data regime of few-shot learning. We have proposed to achieve this via defining an information-retrieval based training objective that simultaneously optimizes all relative orderings of the points in each training batch. We experimentally support our claims for learning efficiency and present promising results on two standard few-shot learning datasets. An interesting future direction is to not only reason about how to best exploit the information within each batch, but additionally about how to create training batches in order to best leverage the information in the training set. Furthermore, we leave it as future work to explore alternative information retrieval metrics, instead of mAP, as training objectives for few-shot learning .