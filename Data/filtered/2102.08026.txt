 In recent years, signal-based authentication has shown great promises, for its inherent robustness against forgery. Electrocardiogram signal, being the most widely studied biosignal, has also received the highest level of attention in this regard. It has been proven with numerous studies that analyzing ECG signals from different persons, it is possible to identify them, with acceptable accuracy. In this work, we present, EDITH, deep learning-based framework for ECG biometrics authentication system. Moreover, we hypothesize and demonstrate that Siamese architectures can be used over typical distance metrics for improved performance. We have evaluated EDITH using commonly used datasets and outperformed the prior works using fewer number of beats. EDITH performs competitively using just single heartbeat and can be further enhanced fusing multiple beats . Furthermore, the proposed Siamese architecture manages to reduce the identity verification Equal Error Rate to case study of EDITH with real-world experimental data also suggests its potential as practical authentication system. Index terms — Authentication, Biometrics, ECG-ID, Convolutional Network, Siamese Network. Introduction Since the dawn of civilization, the human being has been storing valuable objects ensuring secrecy and security. Mechanisms such as keys or combination locks have been used to serve such purposes for centuries. However, with the development of information our valuable possessions have mostly been shifted to the digital domain. Besides, various personalized services and amenities have also received similar transformation. The concern of security and secrecy still applies; thus various measures are adopted to allow only authorized access. to the concepts of keys and combination locks, in the digital domain, tokens and passwords have been being used for decades. But they come with the risk of being lost or stolen, bringing in the peril of malicious access . Therefore, multitude of research works have been conducted for decades to develop resilient authentication systems. There exist lot works in the literature that focused on behavioral modalities, such as voice , keystroke , gait etc. However, the behavioral authentication systems are often cumbersome and unreliable. This has made biometric authentication systems more preferred, which include fingerprint , retina , facial recognition etc. Despite being widely adopted, these methods also face serious issue of forgery attacks . In recent years, signals have also demonstrated great potential in authenticating individuals. signals are in fact more preferable to other means of authentication as they are hard to counterfeit and also the individuals need to present themselves to record such signals . Among various signals electrocardiogram , and photoplethysmography are the most prominent. Compared to most signals, PPG and ECG signals can be obtained relatively easily, for example, using simple finger sensors . Furthermore, ECG signals are more resilient against noises, compared to PPG or EEG, which has made ECG more preferable as biometric system . As result ECG bands , low power circuits and light-weight algorithms have been developed for ECG based authentication systems. Furthermore, with the recent introduction of ECG sensors in commercial products like the Apple Watch and Samsung Galaxy Watch, the possibility of widespread adoption of ECG biometrics has opened . In this paper, we propose EDITH, which is deep learning-based ECG biometric authentication system, EDITH. EDITH comprises novel convolutional network architecture based on multiresolution analysis, employing MultiRes blocks and Spatial Pyramid Pooling in coherent conjunction. To satisfy the reliance on accurate R-peak detection, we develop deep learning-based R-peak detector. Furthermore, we propose novel Siamese architecture incorporating both Euclidean and Cosine similarity distance measures. EDITH has been evaluated on benchmark datasets and have consistently outperformed the prior works.  Related Works The pioneering work of Biel et al. deserves the most credit for establishing the potential of ECG signals for human identification. Ever since this pilot study, ECG, as biometric has drawn widespread interest, resulting in plethora of novel approaches . The different ECG based authentication systems can broadly be divided into two classes, fiducial and non-fiducial. The fiducial methods rely on extracting various fiducial landmarks from the ECG signal. For such approaches, different sub-waves like R, P, are extracted, and features related to various amplitude, difference, or delay terms are computed . However, these methods are often criticized for over-reliance on the fiducial points, slight misdetection of which results in an erroneous identification . The inability of working with noisy or variable heart-beats seriously these methods. Moreover, for satisfactory performance, manual intervention is often required, which further diminishes their utility . To overcome the of the fiducial methods, subsequently, non-fiducial approaches were investigated. These methods analyze the entire or segment of the signal to extract several features . Among the diverse exploration of nonfiducial methods, the use of Kalman filter , Fourier or Wavelet transformation , and generalized S-transformation are the most notable ones. Recently, several deep learning-based methods have also been proposed for ECG biometrics. Such methods have demonstrated much greater accuracy in identification compared to the traditional approaches. Although such methods often rely on the detection of R-peaks for proper alignment of the signals, they still circumvent the ﬂaws of the fiducial approaches. Since R-peak is the highest and sharpest point of ECG and the QRS complex is less likely to be affected physical movement or emotion, they are popularly used for ECG signal alignment . Convolutional networks, ranging from vanilla architectures , multi-resolutional models , residual models have all demonstrated superior accuracy in this context. Similarly, recurrent network models such as Long Short-Term Memory or Gated Recurrent Unit have also been successfully applied for precise individual authentication.  Motivations and High Level Considerations Our primary objective is to develop deep learning model capable of reliable ECG authentication. Instead of using off-the-shelf networks and transfer learning, we analyzed the specific challenges that come with this problem and designed our model accordingly.  Compact Model with Broad Receptive Field The feasibility of deploying an ECG biometric system depends on the model being lightweight. This requirement is imposed for several reasons. Firstly, the end goal would be to run the model on wearable devices, as smartwatches now carry ECG sensors. Since the typical wearable processors can not be made powerful due to space and power constraints, making the model simple is crucial. Secondly, when new users register to the system, they are likely to provide only few ECG samples, therefore it would be diﬃcult to obtain reasonable amount of samples to properly fine-tune deep network. Although it is motivating and suitable to design shallow network, such networks are likely to fall behind in performance. Deep networks have broader field of vision at the later layers and thus they can make much complex inferences. This ability to analyze  a larger portion of the complex featuremaps comes at the cost of network complexity. However, in recent times few approaches have been proposed to mitigate this. MultiRes blocks can be used interchangeably with convolutional layers, providing wider receptive field through multiresolution analysis, in compact form. In terms of pooling, Spatial Pyramid Pooling can be used to leverage pooling operations from different points of view, as if they were computed at much deeper layers. Therefore, we use MultiRes blocks and SPP layers to design compact network with the essence of broad field of view found in deeper networks. More details are presented in Section .  Adapting Siamese Network Distance In order to solve the problem of ‘Identity Verification’, the standard pipeline is to extract features or embeddings using deep learning model and compute the distance with the templates, based on thresholding which the verification is performed. The distances are computed using either Euclidean distance or cosine similarity . Cosine similarity has been the defacto standard in identity verification. To investigate the difference in applicability between the two, we extract embeddings from ECG-ID dataset signals and measure both the cosine distance and or Eucledian distance, and plot histograms . It is evident that distance has comparatively more overlaps, thus the use of cosine similarity is justified. However, when we plot both the distances in histogram, separately for genuine and imposter comparisons, we see different view. In both the figures, the expected region is pointed black border and it can be observed that cosine distance is better at distinguishing the genuine ones whereas distance is more suitable for identifying imposters. Therefore, we design novel Siamese network modeling both the two types of distance. We hypothesize that an adaptive combination of both the distances, learned the Siamese network will be capable of verifying identities better. More details have been presented in Section . Multiple Beat Fusion Although we would like to authenticate the user using single heartbeat signal, just like single fingerprint reading can do, it fails to achieve satisfactory performance. Not only single heartbeat signal contains very little information, but also it succumbs to deviation due to several variabilities. As result, in the literature, the common practice is to work with multiple beats and it greatly boosts the authentication accuracy . When it comes to working with multiple beats, two approaches are followed in general. One approach is to merge multiple beats together and train the model with longer signals . The second approach is to train the model using single beats but during inference, multiple consecutive beats are taken as input and majority voting of their predictions is provided as the final output . In our work, we followed the second approach, i.e., we train the model with single heartbeats but during inference, we have the option to ensemble results from multiple consecutive heartbeats using majority voting. Our rationale is based on few considerations. Firstly, training the model on longer signals would require the model to be more complex but we are less likely to have suﬃcient data to train such model properly. Secondly, not only it is known that ensemble methods always boost the accuracy, but also this provides us with the ﬂexibility to work with single beat if needed . And most importantly, if even one beat deviates from the standard due to some reasons, it will negatively affect the entire merged multibeat signal , but ensembling predictions from multiple beats will be comparatively immune to such inconsistencies. This is our primary motivation, to make the method more robust.  Problem Definition The problem of individual authentication using ECG signals can be divided into two tasks .  Closed Environment Identification For the closed environment identification problem, among fixed set of persons, . The task is to develop model Mcei that is capable of identifying the person pi when given an ECG signal of that person Sigi. This task, eventually reduces into multi-class classification problem.  Identity Verification One major drawback of the former task is that when new individual is introduced to the authentication system, the entire pipeline needs to be retrained with the new data. This becomes cumbersome for practical usage. Therefore, an alternate task has also been studied in the literature. In the identity verification task, we are given an independent dataset of persons and based on certain portion of data, termed as ‘enrollment data’ we generate templates for  them. Then, the task is to match the remaining data, termed as ‘evaluation data’ with the templates and verify the identity. To match sample Siga with the template tb, i.e. measuring the similarity or dissimilarity, distances like cosine or Euclidean distances are used.  Datasets In order to develop, experiment, and evaluate the EDITH system, we consider different public datasets, which are the main ECG datasets used in ECG based authentication purposes in the literature . Below we brieﬂy describe these datasets .  ECG-ID The ECG-ID database is one of the few databases that was developed for the sole purpose of investigating ECG-based authentication. Therefore, this dataset is the defacto choice for researchers to work with ECG biometrics . In this dataset, seconds long ECG signal from lead was recorded from persons. The signals were digitized at Hz with resolution over nominal mV range. For each subject, to session data were collected. Thus it is the common practice among researchers to consider only sessions for each person to avoid any data imbalance issue , which is also considered in this study.  MIT-BIH Arrhythmia Although the MIT-BIH Arrhythmia dataset is centered around arrhythmia detection, this dataset has been used to benchmark biometric authentication accuracy in several works. This dataset contains ECG signals from subjects suffering from wide variety of arrhythmias. The recordings were digitized at Hz sampling rate with resolution over mV range of signals. The ECG signals were recorded in two channels, however, only the first channel was used for this study.  PTB Diagnostic ECG Database The PTB Diagnostic ECG database contains records from subjects, out of which individuals are healthy. Thus, for benchmarking authentication performances only this control group has been used in the literature . This dataset contains ECG records from leads . The signals are digitized in Hz sampling frequency with bits resolution. The ECG signals from the Frank lead was used in this study.  MIT-BIH NSRDB The NSRDB database is part of the MIT-BIH Arrhythmia database . This database contains signals from subjects, who are free from arrhythmias or other abnormalities. This database mostly contains Normal Sinus Rhythm, thus the name NSRDB. beats from each of the individuals were considered for this study. All the datasets were collected from PhysioNet and were resampled to Hz . Notebly all these datasets were already filtered, thus no additional filtering or preprocessing was applied. However, when applying the proposed method to different data source, some preprocessing may be necessary to remove noise and artifacts. An overview of the datasets has been presented in Table In order to perform cross-session evaluation, we used another dataset, CYBHi .  CYBHi Dataset The CYBHi dataset was developed to evaluate ECG biometrics, particularly in multisession scenario. The dataset contains two types of data, short-term and long-term. In the long-term dataset, ECG signals from individuals are presented which are collected over period of several days. The ECG signals were collected from finger using sampling frequency of kHz.  Proposed Method The proposed EDITH pipeline takes continuous ECG signal as input and detects the R-peaks, based on which the beats are extracted. Each heartbeats is then passed through convolutional neural network when dealing with the closed environment identification task. On the contrary, to perform the identity verification task they are compared with the stored templates using siamese network. Furthermore, the results from multiple beats can be ensembled for improving the performance. An overview of the pipeline is presented in Fig. .  R-peak Detection The first step of the proposed method is the peak detection. It is often attributed that peaks being the highest and sharpest point are trivial to detect . However, for diverse set of ECG signals, this is not always the case. As has been demonstrated recent works , popular beat detection algorithms are not consistently suﬃcient to precisely locate the peaks. It was also demonstrated that when working with single-beat authentication system, proper detection of R-peaks is the most vital step, deviating even slightly often diminishes the performance drastically . Uzair et al. have recently proposed an approach to detect R-peaks using deep learning technique, which outperforms the traditional methods. In that work, U-Net was used to predict the location of R-peaks, from an ECG signal. We have deployed our recently proposed variant of U-net architecture, MultiResUNet model with deep supervision as done in , to predict the R-peaks from continuous ECG signal. MultiResUNet model showed improved performance for applications over the traditional U-Net architecture which incorporates multiresolution analysis and deep supervision is proven to improve the performance of deep networks. The probability map predicted from the model is thresholded and further refined some post-processing. It is expected that R-peaks will be considerably distant from one another. Thus, to minimize false positives, we ignore nearby predicted beats and replace them with the median prediction. To reduce false negatives we empirically select suitable threshold value.  Beat Segmentation After the R-peaks have been detected, the individual heartbeats are segmented from the ECG signal, following the approach in . The beats are aligned based on the location of the R-peaks to provide standardized pattern for the model. We take window size of where samples, and include and samples before and after the R-peak respectively. Furthermore, the extracted heartbeat signals were Z-score normalized .  Network Architecture Developing deep learning model for single heartbeat based authentication system is challenging and diﬃcult for two major reasons . Short signals will be affected pooling layers, for losing probable significant information. Furthermore, small dataset prevents us from working with deeper networks. Consequently, pooling operations have been avoided in the literature for this purpose and multiresolution analysis has rather been leveraged instead . Therefore, in our proposed model, we have resorted to using somewhat shallow network, incorporating multiresolution analysis with pooling operations. We were motivated to use the MultiRes block as proposed in and spatial pyramid pooling . The network architecture has been presented in Fig. . The MultiRes block presents compact form of Inception block and incorporates residual connection for aiding in the gradient ﬂow . Both inception block and residual connections have demonstrated favorable outcome in ECG based biometrics. We place convolutional layers having filters of size and respectively and concatenate their featuremaps, which in essence factorizes larger convolutional operations using smaller ones. Since we are using shallow network, we consider convolutional filters of size to expand the receptive field. We avoid intermediate pooling operations to prevent loss of information, but after performing the convolutions for the sake of summarization of the featuremaps, max pooling is performed. To exploit the multiresolutional nature of our analysis further, we perform spatial pyramid pooling instead of the vanilla pooling operation. Three pooling operations, with windows of size of and are performed in parallel and the outputs are concatenated. Finally, fully connected layer with neurons is introduced to generate the embedding for the input signals. This layer is further regularized with dropout of to avoid overfitting . All the layers used in this network are batch-normalized and activated Rectified Linear Unit activation. Closed Environment Identification For the Closed Environment Identification task, the target is to identify predefined number of individuals. Therefore, the problem reduces to Multi-Class Classification problem. We simply include softmax layer on top of our proposed network to perform  this . The number of neurons at the output layer is kept equal to npersons, i.e. the total number of subjects in the closed environment being considered.  Identity Verification For the identity verification task, on the other hand, we do not have any fixed number of individuals as our targets; rather set of templates are computed to compare with. In the literature, for this task, typically group of subjects is used to train deep learning model, which is then used to generate embeddings, removing the top layers. Then, set of enrollment signals is considered, containing few signals from each of the individuals to evaluate the performance of the method. Templates are computed for the individuals taking an average of the embeddings of that individual. Distance measures like Euclidean distance or cosine similarity are widely used in the literature, to match test signals with the templates. We hypothesize that such distance measures may not be capable of weighting the individual features of the embedding properly, and thus an adaptive method may be more suitable. To this end, we propose Siamese architecture to compute the similarity between templates and signal embeddings. Siamese networks employ two identical sub-networks to compare the similarity of two input samples, and they have long history in performing verification tasks . The weights of the two sub-networks are shared together, and hence, they are likely to predict similar featuremaps for similar input samples. They can be further compared together, usually computing the squared differences between the individual pairs of features. However, drawing motivation to emulate the popular Cosine similarity distance metric used for this purpose, in addition to computing the squared differences, we also explore the eﬃcacy of multiplying the individual pairs of features. We term this as Product Proximity between the features, as similar valued feature pairs will yield high multiplication output, whereas feature pairs with dissimilar values will result in low score. Our intuition is that the way squared difference is intended to model eucledian distance, Product Proximity of the features will represent correlation or Cosine similarity like metrics. In addition to working with them separately, we also merge them together and analyze them in fully connected layers . Hence, we take our proposed identification network, pretrained on disjoint set of individuals. The top softmax layer and the dropout layers are then discarded and the top ReLU activation is replaced with Sigmoid activation instead so that we can obtain the embeddings in the range of ∼ . We take another identical sub-network so that the embeddings for two signals can be computed in parallel. After the embeddings are computed, we apply two different types of distance measures between the individual features as mentioned earlier and we further perform combination of the two. The  distance measures are further analyzed the subsequent fully connected layers and merging them we predict similarity score between ∼ . The model architecture for the identity verification task has been presented in Fig. .  Experimental Setup The experiments have been conducted in server computer with Intel Xeon CPU, GB RAM, and NVIDIA TESLA GPU. We implemented the EDITH pipeline in Python programming language. The deep learning models were implemented in Keras with Tensorﬂow backend . The codes are available in the following github repository. R-peak Detection Model In order to train the R-peak Detection Model, we collected the portion of the ECG signals with ground truth annotation of the R-peaks from the ECG-ID database. The continuous signals were divided into windows of samples, with step size of samples. Each windowed signal is composed of roughly three ECG beats. These windowed signals were considered as input to the R-peak detector model, while the output was pulse train showing the location of R-peaks as and elsewhere. In order to apply deep supervision, as reported in , the pulse trains were subsampled accordingly. of the data was used to train the model and the remaining was used as validation data. The model was trained for epochs, minimizing binary cross-entropy loss using the Adam optimizer .  Closed Environment Identification Model For the Closed Environment Identification model, we performed stratified crossvalidation as well as testing using train-val-test split. Thus the data were divided and the CNN models were trained accordingly. We trained the models for epochs, minimizing categorical crossentropy loss using Adam optimizer . Notably, except for the PTB dataset, convergence were reached before epochs for all. The models were evaluated using the standard accuracy metric as follows . In addition, we also compute other standard evaluation metrics. Identity Verification Model We trained the base model using of the individuals from the ECG-ID dataset. The training signals were paired with each other to form training samples for the Siamese model. As we have too many mismatched samoles in comparison to matched samples, we used Synthetic Minority Oversampling Technique to generate synthetic data for matching cases. times synthetic samples were generated for each person as trade-off between data imbalance and computation requirements. This enables the model to learn which patterns are discriminative of matching cases. of the signal pairs were used to train the model and the rest were used as validation data. Since we predict continuous variable as the outcome in this model the Adam optimizer was used to minimize the Mean squared error loss. The model was trained for epochs to predict for mismatches and for matches. For evaluation, the test data was later split into enrollment and evaluation data. The enroll signals were used to generate embeddings, and for each subject, the mean or centroid of the embeddings was considered as the template. The evaluation signals were matched with the templates for each individual, and based on threshold the matchmismatch decision was made. Correspondingly the standard evaluation metrics, namely False Acceptance Rate , False Rejection Rate , and Equal Error Rate were computed. Results and Discussions R-peak detection EDITH Although detection of R-peaks has not been our primary focus, it turns out that our proposed R-peak detection model detects R-peaks with reliable performance. Despite being an auxiliary step of our pipeline, it is on par with the popular R-peak detection algorithms , and outperforms most both quantitatively and qualitatively. To present comparison with the existing algorithms, we detect the R-peaks on the entire ECG-ID and benchmark the various algorithms based on the ground truth annotations provided therein. The results are presented in Table . We consider the  false positives and false negatives as falsely predicted peaks and actual peaks that were missed respectively. Besides, to further assess the time lag of R-peak detection, we propose metric termed ‘Temporal Error’. Suppose, certain R-peak, ri was at timestamp ti and the method predicts it at timestamp ˆti, the Temporal Error is defined as, Temporal Error The proposed R-peak detector achieves the least number of false positives, and quite remarkably negligible Temporal Error. This ensures proper segmentation of the heartbeats for authentication in the next stage of our pipeline. In terms of qualitative assessment of the R-peak detection, we select an ECG signal from the ECG-ID dataset and detect the R-peaks using the different methods. The comparative analysis of the R-peak along with the manual ground-truth annotation is presented in Fig. . Interestingly, the proposed R-peak detector accurately detects all the peaks, whereas other methods predict false peaks or miss R-peaks, and usually yield high time lag between the detected and the actual peak. The superior performance of the proposed method is further illustrated in Fig. where similar observations can be drawn over the segmented beats based on the detected R-peaks, which is the primary concern in this context. It is evident that the beats detected the proposed approach are properly aligned, almost identical to the ground-truth. On the other hand, the traditional methods sometimes not only missed beats but also failed in aligning them properly. Notably, missing beats as well as misaligned ones adversely affects the identification accuracy as pointed out in . Furthermore, we used our R-peak detection model to detect R-peaks from the PTB and NSRDB datasets, without any modification or fine-tuning. In both cases, our model was capable of detecting the R-peaks reliably. However, for the lack of manual groundtruth annotation, it was not possible to evaluate the performance directly. Nevertheless, the high level of accuracy in identifying the individuals implicitly points out to the overall robustness of the R-peak detector.  Comparative Evaluations for the Closed Environment Identification Closed Environment Identification In order to assess the eﬃcacy of EDITH for the closed environment identification task, we follow the two traditional evaluation schemes, stratified cross-validation, and data split. EDITH manages to accurately identify the individuals consistently in all datasets. The accuracy plots of EDITH across the four datasets are shown in Fig.  . It can be noticed that for cross-validation, the accuracy is slightly higher, which could be due to the availability of additional training data. Nevertheless, in all the experiments, EDITH achieves remarkable performance in the closed environment identification. In addition, we also present the standard classification metrics in Table . It can be observed that for MIT-BIH, PTB, and NSRDB datasets, the proposed method achieves balanced precision and recall scores which is also reﬂected at the high . Although in the ECG-ID dataset recall is slightly worse, the precision is comparatively higher. This signifies that the method reduces false positives at the cost of false negatives. This proves beneficial as for an authentication system restricting unauthorized access is more concerning. Nevertheless, the recall, i.e. authorized access can be improved ensembling the result of multiple beats. Fusing Multiple Beats for Improved Performance In the literature, it has been demonstrated that combining multiple beats greatly increases authentication accuracy . This is expected due to the information gain from the other beats. Therefore, we have experimented with performing majority voting over  multiple consecutive beats to further improve the performance of EDITH. From the results presented in Fig. it can be observed that increasing the number of beats significantly improves the performance. We have our analysis up to beats, as prior studies have presented this as an optimal trade-off between performance and practicality . In this regard, EDITH shows great promise achieving the perfect accuracy using only and beats. In Table we have presented comparative analysis of closed environment identification performance of EDITH using the split on the four different datasets, along with the state-of-the-art results. Here, we only list the recent, top-performing deep learning-based approaches and it can be observed that EDITH outperforms them either scoring higher or achieving the same performance with less number of beats. Cross-Session Evaluation For biometric authentication protocols, it is more meaningful to evaluate them using multi-session data, i.e., training the models with the data from particular session and test them with data from different session. Very few ECG databases provide us with multi-session data, ECG-ID and CYBHi are two of them. Therefore, we analyze the cross-session accuracy using these two datasets. The result for cross-session analysis is presented in Table . As it is intuitive and also reported , identification accuracy sharply falls when tested on data from different session, due to several variabilities. Despite that our model handles this quite well, as the accuracy on the ECG-ID dataset is still above . However BYCHi dataset being more complex one, our performance falls below still, it is over better than the previous best-performing work . For the ECG-ID dataset, although performs better in such scenarios, their performance is not balanced, e.g. for some individuals, their model fails whereas our individual level accuracies are always over .  Identity Verification Experiments Siamese Architecture vs Cosine Similarity As mentioned in the earlier sections, following the standard protocols to benchmark the identity verification task, we first train model using subset of individuals from the ECG-ID dataset and use that model to predict embeddings for the test signals. The test signals are further divided into two parts, enrollment and evaluation. It is common practice to keep data for enrollment and use the rest for evaluation . However, we experimented with different sizes of enrollment data and different numbers of beats. The EER values obtained for the different configurations are presented in Fig. . In addition, we include the results using cosine similarity and the resultant improvements achieved using the Siamese network. From Fig. we can observe that increasing the size of enrollment data at first gradually improves the performance, but then it starts to degrade. This can be explained as follows . If it is too large, this will enforce bias on the matching task . In our experiments, we have found that of the entire data used as the enrollment data yields the best performance. This is reasonable setting, considering the prior works , where data is used. On the other hand, increasing the number of beats almost consistently improves the performance. Again, it is evident that using the proposed Siamese network invariably yields better performance compared to the standard cosine similarity metric. In Table the results obtained the comparative evaluation against the prior approaches are presented, where EDITH achieves the top performance, registering better EER value than others. Case Study We further analyze some cases where EDITH failed in verifying the identities. Fig. presents some relevant examples. For the true acceptance and true rejection cases, the reasons for the correct decision are clearly visible through visual comparison of the input with the template. However, for some cases, signals from other individuals closely mimicked the pattern of the templates . On the contrary, in some situations, the ECG signal from the same person may deviate significantly, and hence, it hardly matches the template . The level of variability and inconsistency in the ECG signals are responsible for such erroneous decisions, this is the primary motivation for analyzing multiple beats instead of single one. Result on Other Datasets We also evaluated our identity verification performance on the other datasets. Since very few works were evaluated on datasets other than ECG-ID, we could not present any meaningful comparison. We present the EER and ROC curves in Fig. and Fig.  respectively. The results here presented are for single beat only with enrollment rate. The scores on other datasets are less than ECG-ID dataset because the model was adapted for ECG-ID dataset and just fine-tuned on the other datasets. Investigation on model interpretability The improved performance of machine learning often comes at the cost of interpretability. However, for security and biometrics applications, interpretation of algorithms is imperative. Not only this will enable us to assert that the model is learning actual significant information, but also it will act as defense mechanism against spoofing or counterfeiting attacks. Recently, considerable focus has been put on interpreting the deep learning models, instead of considering them merely as black-boxes . With the above backdrop, we investigate the interpretability of our proposed method. First of all, to analyze the discriminative features extracted the model for random signal from the ECG-ID database , we visualize the feature maps computed the first convolutional layer . Although the earlier feature maps alone do not suﬃciently signify, still we can obtain some rough ideas about the features. For example, it can be observed that some filter seeks for the , , regions, while some filter just merely denoise or invert the signal. Saliency maps provide further insights into the interpretability of the models, which highlight the parts of the signal that inﬂuence the most in making the predictions. Saliency maps can be generated computing the gradient of the loss function with respect to the individual timestamps of the input signal . For example, for an sample long input signal, , we can compute the saliency map, as si , where is the value of the loss function.  presents the saliency maps of different signals from different persons from the ECG-ID database, where the regions with high saliency scores are colored in red. For Person we have clear region and vague region, but for Person we observe the opposite. Thus, in order to discriminate them, one would look for the and regions, which are the ones sought the model as demonstrated in Fig. . Since signals from both the person have conspicuous peak, the model emphasizes that as well. However, for Person the peak is less dominant, thus the model has put little emphasis there. This analysis indicates the model’s capability of identifying the intelligible fiducial landmarks and making the prediction accordingly. Evaluation with Real-World Data In order to assess the practical utility of the proposed method, we also perform trial. We collect ECG signals from persons using HealthyPi . This kit contains an ECG front end of Texas Instruments with resolution, having signal-to-noise ratio of dB. The ECG signals are recorded at Hz, which are upsampled to Hz using bicubic interpolation for our analysis. We take the R-peak detector model and the closed environment identification model trained on the ECG-ID database and investigate their applicability with this completely different source of data. The model successfully detects the R-peaks, leading to proper segmentation of the beats. In Fig. we have presented an overlapping plot of the ECG beats for different persons and it can be observed that all the beats have been properly segmented except one. Similarly, we pass the detected beats to the identification model and compute the embeddings. We present t-distributed stochastic neighbor embedding plot of the embeddings in Fig. . It can be observed that the embeddings from individual persons are clustered together, which indicates that basic fine-tuning or template matching would be able to identify the persons. Therefore, this study demonstrates the potential of EDITH in real-world authentication applications. It is indeed promising that the model was able to generalize to this extent, despite the data being captured using different device through different protocol, and that too with different sampling rate and quantization. To sum up, owing to the continual adversarial attacks and forgery attempts, security researchers are in constant pursuit of more reliable and robust biometric authentication systems. Leveraging signals seems to be the next pivotal step in this regard, due to the inherent defense mechanism thereof against forgery. The proposed EDITH system is step towards attaining this objective. We have demonstrated superior performance of EDITH over the existing methods in different tasks using various datasets. Furthermore, EDITH circumvents certain of the contemporary methods, for instance, inconsistent R-peak detection and the need for longer ECG signals.  Conclusion The domains of security and biometrics have always been an arms race. As efforts were put in developing more advanced authentication systems, similar level of striving was in place to forge them. For example, facial biometrics was introduced to overcome the of face scanning biometrics, as mere images were able to spoof them. But still, those systems were still vulnerable to face model based counterfeiting attacks . As result, researchers from this domain have not only been attempting to improve the existing systems but also are constantly looking for more reliable alternatives. In this regard, signal based biometrics, bring in greater promises as they are comparatively diﬃcult to fake. In this work, we have presented EDITH, biometric authentication system based on ECG signals and deep learning techniques. Similar to EDITH was developed to work with single heartbeat, but its performance can be enhanced merging multiple beats. We propose novel architecture leveraging multiresolution analysis. As result, EDITH outperforms existing methods achieving better accuracy with less number of beats. Furthermore, we present competitive alternative to the widely adopted Cosine similarity-based identity verification. The proposed novel Siamese architecture, incorporating both Euclidean and Cosine-like distances consistently surpasses Cosine similarity-based template matching. Another remarkable achievement of EDITH lies in its ability to learn the fiducial points of ECG signals without any explicit supervision. Furthermore, trial with real-world ECG data corroborates the potential of EDITH as practical authentication system. Perhaps, signal based biometrics are yet to be suﬃcient for real-world deployment. Nevertheless, for the favorable characteristics they bring in, such means of authentication is worth investigating further. The future directions of this research can be manifold. It is indeed true that owing to the variability and diversity using single heartbeat is unlikely to offer reliable authentication. Despite this, it deserves further investigation to attain the holy grail of single beat authentication. Analysis of variation in ECG signals with time along with different states of the individual is another avenue that deserves more attention to make authentication more robust and steadfast.