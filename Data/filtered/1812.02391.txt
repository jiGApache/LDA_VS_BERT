 Meta-learning has been proposed as framework to address the challenging few-shot learning setting. The key idea is to leverage large number of similar few-shot tasks in order to learn how to adapt base-learner to new task for which only few labeled samples are available. As deep neural networks tend to overfit using few samples only, meta-learning typically uses shallow neural networks , thus its effectiveness. In this paper we propose novel few-shot learning method called meta-transfer learning which learns to adapt deep NN for few shot learning tasks. Specifically, meta refers to training multiple tasks, and transfer is achieved learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task meta-batch scheme as an effective learning curriculum for MTL. We conduct experiments using and recognition tasks on two challenging few-shot learning benchmarks . Extensive comparisons to related works validate that our meta-transfer learning approach trained with the proposed HT meta-batch scheme achieves top performance. An ablation study also shows that both components contribute to fast convergence and high .  . Introduction While deep learning systems have achieved great performance when sufficient amounts of labeled data are available , there has been growing interest in reducing the required amount of data. Few-shot learning tasks have been defined for this purpose. The aim is to learn new concepts from few labeled examples, e.g. learning . While humans tend to be highly effective in this context, often grasping the essential connection between new concepts and their own knowledge and experience, it remains challenging for machine learning approaches. E.g., on the dataset, state-of-the-art method achieves only accuracy for learning, compared to for the all-class fully supervised case . Few-shot learning methods can be roughly categorized into two classes . Data augmentation is classic technique to increase the amount of available data and thus also useful for few-shot learning . Several methods propose to learn data generator e.g. conditioned on Gaussian noise . However, the generation models often underperform when trained on few-shot data . An alternative is to merge data from multiple tasks which, however, is not effective due to variances of the data across tasks . In contrast to data-augmentation methods, meta-learning is task-level learning method . Meta-learning aims to accumulate experience from learning multiple tasks , while base-learning focuses on modeling the data distribution of single task. state-of-theart representative of this, namely Model-Agnostic MetaLearning , learns to search for the optimal initialization state to fast adapt base-learner to new task . Its task-agnostic property makes it possible to generalize to few-shot supervised learning as well as unsupervised reinforcement learning . However, in our view, there are two main of this type of approaches their effectiveness . For example, for the miniImageNet dataset , MAML uses shallow CNN with only CONV layers and its optimal performance was obtained learning on tasks. In this paper, we propose novel meta-learning method called meta-transfer learning leveraging the advantages of both transfer and meta learning . In nutshell, MTL is novel learning method that helps deep neural nets converge faster while reducing the probability to overfit when using few labeled training data only. In particular, “transfer” means that DNN weights trained on largescale data can be used in other tasks two light-weight neuron operations .e. β. “Meta” means that the parameters of these operations can be viewed as hyper-parameters trained on few-shot learning tasks . Large-scale trained DNN weights offer good initialization, enabling fast convergence of metatransfer learning with fewer tasks, e.g. only tasks for miniImageNet , times fewer than MAML . Lightweight operations on DNN neurons have less parameters to learn, e.g. less than if considering neurons of size , reducing the chance of overfitting. In addition, these operations keep those trained DNN weights unchanged, and thus avoid the problem of “catastrophic forgetting” which means forgetting general patterns when adapting to specific task . The second main contribution of this paper is an effective meta-training curriculum. Curriculum learning and hard negative mining both suggest that faster convergence and stronger performance can be achieved better arrangement of training data. Inspired these ideas, we design our hard task meta-batch strategy to offer challenging but effective learning curriculum. As shown in the bottom rows of Figure conventional meta-batch contains number of random tasks , but our HT meta-batch online re-samples harder ones according to past failure tasks with lowest validation accuracy. Our overall contribution is thus three-fold .  . Related work Few-shot learning Research literature on few-shot learning exhibits great diversity. In this section, we focus on methods using the supervised meta-learning paradigm most relevant to ours and compared to in the experiments. We can divide these methods into three categories. Metric learning methods learn similarity space in which learning is efficient for few-shot examples. Memory network methods learn to store “experience” when learning seen tasks and then generalize that to unseen tasks. Gradient descent based methods have specific meta-learner that learns to adapt specific base-learner through different tasks. E.g. MAML uses meta-learner that learns to effectively initialize base-learner for new learning task. Meta-learner optimization is done gradient descent using the validation loss of the base-learner. Our method is closely related. An important difference is that our MTL approach leverages transfer learning and benefits from referencing neuron knowledge in pre-trained deep nets. Although MAML can start from pre-trained network, its element-wise fine-tuning makes it hard to learn deep nets without overfitting . Transfer learning What and how to transfer are key issues to be addressed in transfer learning, as different methods are applied to different source-target domains and bridge different transfer knowledge . For deep models, powerful transfer method is adapting pre-trained model for new task, often called fine-tuning . Models pretrained on large-scale datasets have proven to generalize better than randomly initialized ones . Another popular transfer method is taking pre-trained networks as backbone and adding high-level functions, e.g. for object detection and recognition and image segmentation . Our meta-transfer learning leverages the idea of transferring pre-trained weights and aims to meta-learn how to effectively transfer. In this paper, large-scale trained DNN weights are what to transfer, and the operations of Scaling and Shifting indicate how to transfer. Similar operations have been used to modulating the per-feature-map distribution of activations for visual reasoning . Some few-shot learning methods have been proposed to use pre-trained weights as initialization . Typically, weights are fine-tuned for each task, while we learn meta-transfer learner through all tasks, which is different in terms of the underlying learning paradigm. Curriculum learning & Hard sample mining Curriculum learning was proposed Bengio et al. and is popular for multi-task learning . They showed that instead of observing samples at random it is better to organize samples in meaningful way so that fast convergence, effective learning and better generalization can be achieved. Pentina et al. use adaptive SVM classifiers to evaluate task difficulty for later organization. Differently, our MTL method does task evaluation online at the phase of episode test, without needing any auxiliary model. Hard sample mining was proposed Shrivastava et al. for object detection. It treats image proposals overlapped with ground truth as hard negative samples. Training on more confusing data enables the model to achieve higher robustness and better performance . Inspired this, we sample harder tasks online and make our MTL learner “grow faster and stronger through more hardness”. In our experiments, we show that this can be generalized to enhance other meta-learning methods, e.g. MAML .  . We introduce the problem setup and notations of metalearning, following related work . Meta-learning consists of two phases . meta-training example is classification task sampled from distribution . is called episode, including training split to optimize the base-learner, and test split to optimize the meta-learner. In particular, meta-training aims to learn from number of episodes sampled from . An unseen task Tunseen in metatest will start from that experience of the meta-learner and adapt the base-learner. The final evaluation is done testing set of unseen datapoints unseen. Meta-training phase. This phase aims to learn metalearner from multiple episodes. In each episode, metatraining has two-stage optimization.  is called base-learning, where the cross-entropy loss is used to optimize the parameters of the base-learner. contains feed-forward test on episode test datapoints. The test loss is used to optimize the parameters of the meta-learner. Specifically, given an episode , the base-learner is learned from episode training data and its corresponding loss LT After optimizing this loss, the baselearner has parameters . Then, the meta-learner is updated using test loss LT After meta-training on all episodes, the meta-learner is optimized test losses . Therefore, the number of metalearner updates equals to the number of episodes. Meta-test phase. This phase aims to test the performance of the trained meta-learner for fast adaptation to unseen task. Given Tunseen, the meta-learner teaches the baselearner to adapt to the objective of Tunseen some means, e.g. through initialization . Then, the test result on unseen is used to evaluate the meta-learning approach. If there are multiple unseen tasks , the average result on will be the final evaluation.  . As shown in Figure our method consists of three phases. First, we train DNN on large-scale data, e.g. on miniImageNet , and then fix the low-level layers as Feature Extractor . Second, in the meta-transfer learning phase, MTL learns the Scaling and Shifting parameters for the Feature Extractor neurons, enabling fast adaptation to few-shot tasks . For improved overall learning, we use our HT meta-batch strategy . The training steps are detailed in Algorithm in Section . Finally, the typical meta-test phase is performed, as introduced in Section .  . DNN training on large-scale data This phase is similar to the classic pre-training stage as, e.g., pre-training on Imagenet for object recognition . Here, we do not consider data/domain adaptation from other datasets, and pre-train on readily available data of few-shot learning benchmarks, allowing for fair comparison with other few-shot learning methods. Specifically, for particular few-shot dataset, we merge all-class data for pretraining. For instance, for miniImageNet , there are totally classes in the training split of and each class contains samples used to pre-train classifier. We first randomly initialize feature extractor and classifier , and then optimize them gradient descent as follows, , where denotes the following empirical loss, LD , , e.g. cross-entropy loss, and denotes the learning rate. In this phase, the feature extractor is learned. It will be frozen in the following meta-training and meta-test phases, as shown in Figure . The learned classifier will be discarded, because subsequent few-shot tasks contain different classification objectives, e.g. instead of classification for miniImageNet .  . Meta-transfer learning As shown in Figure , our proposed meta-transfer learning method optimizes the meta operations Scaling and Shifting through HT meta-batch training . Figure visualizes the difference of updating through SS and FT. SS operations, denoted as and do not change the frozen neuron weights of during learning, while FT updates the complete. In the following, we detail the SS operations. Given task , the loss of is used to optimize the current base-learner gradient descent . as we do not update . Note that here is different to the one from the previous phase, the large-scale classifier in Eq. . This concerns only few of classes, e.g. classes, to classify each time in novel few-shot setting. corresponds to temporal classifier only working in the current task, initialized the optimized for the previous task .  is initialized ones and zeros. Then, they are optimized the test loss of as follows, LT , . .  . In this step, is updated with the same learning rate as in Eq. , . Re-linking to Eq. we note that the above comes from the last epoch of base-learning on . Next, we describe how we apply to the frozen neurons as shown in Figure . Given the trained , for its l-th layer containing neurons, we have pairs of parameters, respectively as weight and bias, denoted as . Note that the neuron location will be omitted for readability. Based on MTL, we learn pairs of scalars. Assuming is input, we apply  to as SS , where denotes the element-wise multiplication. Taking Figure as an example of single filter, after SS operations, this filter is scaled then the feature maps after convolutions are shifted in addition to the original bias b. Detailed steps of SS are given in Algorithm in Section . Figure shows typical parameter-level Fine-Tuning operation, which is in the meta optimization phase of our related work MAML . It is obvious that FT updates the complete values of and and has large number of parameters, and our SS reduces this number to below in the example of the figure. In summary, SS can benefit MTL in three aspects. It starts from strong initialization based on large-scale trained DNN, yielding fast convergence for MTL. It does not change DNN weights, thereby avoiding the problem of “catastrophic forgetting” when learning specific tasks in MTL. It is light-weight, reducing the chance of overfitting of MTL in few-shot scenarios.  . Hard task meta-batch In this section, we introduce method to schedule hard tasks in meta-training batches. The conventional metabatch is composed of randomly sampled tasks, where the randomness implies random difficulties . In our metatraining pipeline, we intentionally pick up failure cases in each task and re-compose their data to be harder tasks for adverse re-training. We aim to force our meta-learner to “grow up through hardness”. Pipeline. Each task has two splits, and , for base-learning and test, respectively. As shown in Algorithm line base-learner is optimized the loss of . SS parameters are then optimized the loss of once. We can also get the recognition accuracy of for classes. Then, we choose the lowest accuracy Accm to determine the most difficult class-m in the current task. After obtaining all failure classes from tasks in current meta-batch , we re-sample tasks from their data. Specifically, we assume is the task distribution, we sample “harder” task hard . Two important details are given below. Choosing hard class-m. We choose the failure class-m from each task ranking the class-level accuracies instead of fixing threshold. In dynamic online setting as ours, it is more sensible to choose the hardest cases based on ranking rather than fixing threshold ahead of time. Two methods of hard tasking using . Chosen , we can re-sample tasks hard directly using the samples of class-m in the current task , or indirectly using the label of class-m to sample new samples of that class. In fact, setting considers to include more data variance of class-m and it works better than setting in general.  . Algorithm Algorithm summarizes the training process of two main stages . HT meta-batch re-sampling and continuous training phases are shown in lines for which the failure classes are returned Algorithm see line . Algorithm presents the learning process on single task that includes episode training and episode test, i.e. meta-level update . In lines the recognition rates of all test classes are computed and returned to Algorithm for hard task sampling. Experiments We evaluate the proposed MTL and HT meta-batch in terms of few-shot recognition accuracy and model convergence speed. Below we describe the datasets and detailed settings, followed an ablation study and comparison to state-of-the-art methods.  . Datasets and implementation details We conduct few-shot learning experiments on two benchmarks, miniImageNet and . miniImageNet is widely used in related works . is newly proposed in and is more challenging in terms of lower image resolution and stricter training-test splits than miniImageNet. miniImageNet was proposed Vinyals et al. for fewshot learning evaluation. Its complexity is high due to the use of ImageNet images, but requires less resource and infrastructure than running on the full ImageNet dataset . In total, there are classes with samples of color images per class. These classes are divided into and classes respectively for sampling tasks for meta-training, meta-validation and meta-test, following related works .  is based on the popular object classification dataset . The splits were proposed . It offers more challenging scenario with lower image resolution and more challenging meta-training/test splits that are separated according to object super-classes. It contains object classes and each class has samples of color images. The classes belong to super-classes. Meta-training data are from classes belonging to super-classes. Meta-validation and meta-test sets contain classes belonging to super-classes, respectively. These splits accord to super-classes, thus minimize the information overlap between training and val/test tasks. The following settings are used on both datasets. We train large-scale DNN with all training datapoints and stop this training after iterations. We use the same task sampling method as related works . Specifically, we consider the classification and we sample episodes to contain samples for train episode, and samples for episode test. Note that in the state-ofthe-art work , and samples are respectively used in and settings for episode test. In total, we sample tasks for meta-training , and respectively sample random tasks for meta-validation and meta-test. Please check the supplementary document for other implementation details, e.g. learning rate and dropout rate. Network architecture. We present the details for the Feature Extractor , MTL meta-learner with Scaling and Shifting and MTL base-learner  The architecture of have two options, and commonly used in related works . consists of layers with convolutions and filters, followed batch normalization , ReLU nonlinearity, and max-pooling. is more popular in recent works . It contains residual blocks and each block has CONV layers with kernels. At the end of each residual block, max-pooling layer is applied. The number of filters starts from and is doubled every next block. Following blocks, there is mean-pooling layer to compress the output feature maps to feature embedding. The difference between using and using in our methods is that MTL sees the large-scale data training, but MTL is learned from scratch because of its poor performance for large-scale data training . Therefore, we emphasize the experiments of using MTL for its superior performance. The architectures of and are generated according to the architecture of , as introduced in Section . That is when using in MTL, and also have layers, respectively. The architecture of is an FC layer. We empirically find that single FC layer is faster to train and more effective for classification than multiple layers. .  . Ablation study setting In order to show the effectiveness of our approach, we design some ablative settings . Note that the alternative meta-learning operation to SS is the FT used in MAML. Some bullet names are explained as follows. update . There is no meta-training phase. During test phase, each task has its whole model updated on , and then tested on . FT . These are straight-forward ways to define smaller set of meta-learner parameters than MAML. We can freeze low-level pre-trained layers and meta-learn the classifier layer with high-level CONV layer that is the residual block of .  . Results and analysis Table Table and Table present the overall results on miniImageNet and datasets. Extensive comparisons are done with ablative methods and the state-of-the-arts. Note that tables present the highest accuracies for which the iterations were chosen validation. For the miniImageNet, iterations for and are at and respectively. For the iterations are all at . Figure shows the performance gap between with and without HT meta-batch in terms of accuracy and converging speed. Result overview on miniImageNet. In Table we can see that the proposed MTL with SS , HT meta-batch and achieves the best few-shot classification performance with for . Besides, it tackles the tasks with an accuracy of that is comparable to the state-of-the-art results, i.e. reported TADAM whose model used additional FC layers in the arch. In terms of the network arch, it is obvious that models using outperforms those using large margins, e.g. models have the best result with which is lower than our best. Result overview on . In Table we give the results of TADAM using their reported numbers in the paper . We used the public code of MAML to get its results for this new dataset. Comparing these methods, we can see that MTL consistently outperforms MAML large margins, i.e. around in all tasks; and surpasses TADAM relatively larger number of for and with and respectively for and tasks. MTL vs. No meta-learning. Table shows the results of No meta-learning on the top block. Compared to these, our approach achieves significantly better performance even without HT meta-batch, e.g. the largest margins are for and for on miniImageNet. This validates the effectiveness of our meta-learning method for tackling few-shot learning problems. Between two No miniImageNet update update FT FT FT SS SS Table . Classification accuracy using ablative models, on two datasets. “meta-batch” and ” are used. meta-learning methods, we can see that updating both feature extractor and classifier is inferior to updating only, e.g. around reduction on miniImageNet . One reason is that in few-shot settings, there are too many parameters to optimize with little data. This supports our motivation to learn only during base-learning. Performance effects of MTL components. MTL with full components, SS , HT meta-batch and , achieves the best performances for all few-shot settings on both datasets, see Table and Table . We can conclude that our large-scale network training on deep CNN significantly boost the few-shot learning performance. This is an important gain brought the transfer learning idea in our MTL approach. It is interesting to note that this gain on is not as large as for miniImageNet and . The possible reason is that tasks for meta-train and meta-test are clearly split according to super-classes. The data domain gap is larger than that for miniImageNet, which makes transfer more difficult. HT meta-batch and in our approach can be generalized to other meta-learning models. MAML with HT meta-batch gains averagely on two datasets. When changing deep it achieves significant improvements, e.g. and on miniImageNet. Compared to MAML variants, our MTL results are consistently higher, e.g. ∼ on . People may argue that MAML fine-tuning all network parameters is likely to overfit to few-shot data. In the middle block of Table we show the ablation study of freezing low-level pre-trained layers and meta-learn only the highlevel layers the FT operations of MAML. These all yield inferior performances than using our SS. An additional observation is that performs consistently better than  Speed of convergence of MTL. MAML used tasks to achieve the best performance on miniImageNet. Impressively, our MTL methods used only tasks, see Figure . This advantage is more obvious for on which MTL methods need at most tasks, Figure . We attest this to two reasons. First, MTL starts from the pre-trained . And second, SS needs to learn only parameters of the number of FT when using . Speed of convergence of HT meta-batch. Figure shows MTL with HT meta-batch consistently achieves higher performances than MTL with the conventional metabatch , in terms of the recognition accuracy in all settings; and it is impressive that MTL with HT meta-batch achieves top performances early, after e.g. about iterations for for and for on the more challenging dataset – .  . Conclusions In this paper, we show that our novel MTL trained with HT meta-batch learning curriculum achieves the top performance for tackling few-shot learning problems. The key operations of MTL on pre-trained DNN neurons proved highly efficient for adapting learning experience to the unseen task. The superiority was particularly achieved in the extreme cases on two challenging benchmarks – miniImageNet and . In terms of learning scheme, HT meta-batch showed consistently good performance for all baselines and ablative models. On the more challenging benchmark, it showed to be particularly helpful for boosting convergence speed. This design is independent from any specific model and could be generalized well whenever the hardness of task is easy to evaluate in online iterations.