 Deep neural networks have achieved state-of-the-art results on time series classification tasks. In this work, we focus on leveraging DNNs in the often-encountered practical scenario where access to labeled training data is difficult, and where DNNs would be prone to overfitting. We leverage recent advancements in gradientbased meta-learning, and propose an approach to train residual neural network with convolutional layers as meta-learning agent for few-shot TSC. The network is trained on diverse set of fewshot tasks sampled from various domains such that it can solve target task from another domain using only small number of training samples from the target task. Most existing meta-learning approaches are in practice as they assume fixed number of target classes across tasks. We overcome this in order to train common agent across domains with each domain having different number of target classes, we utilize triplet-loss based learning procedure that does not require any constraints to be enforced on the number of classes for the few-shot TSC tasks. To the best of our knowledge, we are the first to use meta-learning based pre-training for TSC. Our approach sets new benchmark for few-shot TSC, outperforming several strong baselines on few-shot tasks sampled from datasets in UCR TSC Archive. We observe that pre-training under the meta-learning paradigm allows the network to quickly adapt to new unseen tasks with small number of labeled instances. KEYWORDS Time Series Classification, Meta-Learning, Few-Shot Learning, Convolutional Neural Networks INTRODUCTION Time series data is ubiquitous in the current digital era with several applications across domains such as forecasting, healthcare, equipment health monitoring, and among others. Time series classification has several practical applications such as disease diagnosis from time series of parameters , classifying heart arrhythmias from ECG signals , and human activity recognition . Recently, deep neural networks such as those based on long short term memory networks and convolution neural networks have achieved state-of-the-art results on TSC tasks. However, it is well-known that DNNs are prone to overfitting, especially when access to large labeled training dataset is not available. . Few recent attempts aim to address the issue of scarce labeled data for univariate TSC leveraging transfer learning © Copyright held the owner/author . via DNNs, e.g. . These approaches consider pretraining deep network in an unsupervised or supervised manner using large number of time series from diverse domains, and then fine-tune the pre-trained model for the target task using labeled data from target domain. However, these transfer learning approaches for TSC based on pre-training network on large number of diverse time series tasks do not necessarily guarantee pre-trained model that can be quickly fine-tuned with very small number of labeled training instances, and rather rely on ad-hoc fine-tuning procedures. Rather than learning new task from scratch, humans leverage their pre-existing skills fine-tuning and recombining them, and hence are highly data-efficient, i.e. can learn from as little as one example per category . Meta-learning approaches intend to take similar approach for few-shot learning, i.e. learning task from few examples. More recently, several approaches for few-shot learning for regression, image classification, and reinforcement learning domains have been proposed under the gradientbased meta-learning or the “learning to learn" framework, e.g. in . neural network-based meta-learning model is explicitly trained to quickly learn new task from small amount of data. The model learns to solve several tasks sampled from given distribution where each task is, for example, an image classification problem with few labeled examples. Since each task corresponds to learning problem, performing well on task corresponds to learning quickly. Despite the advent of aforementioned pre-trained models for time series, few-shot learning for TSC remains an important and unaddressed research problem. The goal of few-shot TSC is to train model on large number of diverse few-shot TSC tasks such that it can leverage this experience through the learned parameters, and quickly generalize to new tasks with small number of labeled instances. More specifically, we train residual network on several few-shot TSC tasks such that the ResNet thus obtained generalizes to solve new few-shot learning tasks. In contrast to existing methods for data-efficient transfer learning, our method provides way to directly optimize the embedding itself for classification, rather than an intermediate bottleneck layer such as the ones proposed in . Key contributions of this work are .  We propose few-shot UTSC approach training ResNet to solve diverse few-shot UTSC tasks using meta-learning procedure . The ResNet thus obtained can be quickly adjusted on new, previously unseen, UTSC task with few labeled examples per class.   As opposed to fixed classification setting in most existing few-shot methods, our approach can handle multiway classification problems with varying number of classes without introducing any additional task-specific parameters to be trained from scratch such as those in the final classification layer . This allows our approach to leverage the same neural network architecture across diverse applications without introducing any additional task-specific parameters to be trained from scratch.  Since the proposed approach uses triplet loss to learn Euclidean embedding for time series, it can also be seen as data-efficient metric learning procedure for time series that can learn from very small number of labeled instances. In few-shot setting, we demonstrate that vanilla nearest-neighbor classifier over the embeddings obtained using our approach outperforms existing nearest-neighbor classifiers based on the highly effective dynamic time warping classifier and even stateof-the-art time series classifier BOSS . The rest of the paper is organized as follows . We define the problem of few-shot learning for UTSC in Section . We then provide details of the neural network architecture used for training the few-shot learner in Section followed the details of meta-learning based training algorithm for few-shot UTSC in Section . We provide details of empirical evaluation of proposed approach in Section and conclude in Section .  RELATED WORK Several approaches have been proposed to deal with scarce labeled data for TSC, via data augmentation, warping, simulation, transfer learning, etc. in e.g. . Regularization in DNNs, e.g. decorrelating convolutional filter weights has been found to be effective for TSC and avoid overfitting in scarce data scenarios. Iterative semi-supervised learning also addresses scarce labeled data scenario iteratively increasing the labeled set but assumes availability of relatively large amount of data albeit initially unlabeled. In this work, we take different route to deal with scarce labeled data scenarios and leverage gradient-based meta-learning to explicitly train network to quickly adapt and solve new few-shot TSC tasks. Transfer learning using pre-trained DNNs has been shown to achieve better classification performance than training DNNs from scratch for TSC .g. . However, none of these methods are explicitly trained to quickly adapt to target task and tend to rely on ad-hoc fine-tuning procedures. Furthermore, they do not study the extreme case of few-shot TSC . Our approach explicitly pre-trains DNN using triplet loss to optimize for quick adaptation to few-shot task. Moreover, unlike existing methods, our approach directly optimizes for time series embeddings over which the similarity of time series can be defined, and hence can work in kNN setting without requiring the training of additional parameters like those of an SVM in , or those of feedforward final layer in . Several approaches for few-shot learning have been recently introduced for image classification, regression, and reinforcement learning, e.g. . To the best of our knowledge, our work is the first attempt to study few-shot learning for TSC. We formulate the few-shot learning problem for UTSC, and build on top of the following recent advances in deep learning research to develop an effective few-shot approach for TSC . Dynamic time warping and its variants are known to be very robust and strong distance metric baselines for TSC over diverse set of applications . However, it is also wellknown that no single distance metric works well across scenarios as they lack the ability to leverage the data-distribution and properties of the task at hand . It has been shown that k-nearest-neighbor TSC can be significantly improved learning distance metric from labeled examples . Similarly, modeling time series similarity using Siamese recurrent networks based supervised learning has been proposed in . CNNs trained using triplet loss for TSC have been very recently proposed for unsupervised learning in and for supervised learning in . However, to the best of our knowledge, none of the metric learning approaches consider pre-training neural network that can be quickly fine-tuned for new TSC few-shot tasks.  PROBLEM DEFINITION Consider learning problem for UTSC sampled from distribution that requires learning multi-way classifier for test task given only labeled time series instances per class. Rather than training classifier from scratch for the test task, the goal is to obtain neural network with parameters that is trained to efficiently solve several learning tasks sampled from . These tasks are divided into three sets . The training meta-set is used to obtain the parameters the validation meta-set is used for model selection , and the testing meta-set is used only for final evaluation. Each task instance ∼ in and consists of labeled training set of univariate time series . . . . . . , where is the number of univariate time series instances for each of the classes. Ignoring the suband super-scripts, each univariate time series . . . with for . . . where is the length of time series, and is the class label. Unlike the tasks in and which only contain training set, each task in also contains testing set . . . . . .   apart from training set . The classes in and are the same while  classes across tasks are, in general, different. For any from , the goal is to estimate the corresponding label using an updated version of obtained fine-tuning the neural network using the labeled samples from . In other words, the training set of task is used for fine-tuning the neural network parameters while the corresponding testing set of the task is used for evaluation. It is to be noted that the tasks in the three meta-sets correspond to time series from disjoint sets of classes, i.e. the classes in any task in training meta-set are different from those of any task in validation meta-set, and so on. In practice, we sample the tasks from diverse domains such as electric devices, motion capture, spectrographs, sensor readings, ECGs, simulated time series, etc. taken from the UCR TSC Archive . Each dataset, and in turn tasks sampled from it, have different notion of classes depending upon the domain, different number of classes and different   NEURAL NETWORK As shown in Figure we consider ResNet consisting of multiple convolutional blocks with shortcut residual connections between them, eventually followed global average pooling layer such that the network does not have any feedforward layers at the end. Each convolutional block consists of convolutional layer followed batch normalization layer which acts as regularizer. Each BN layer is in turn followed ReLU layer. We omit further architecture details and refer the reader to . In order to quickly adapt to any unseen task, the neural network should be able to extract temporal features at multiple time scales and should ensure that the fine-tuned network can generalize to time series of varying lengths across tasks. We, therefore, use filters of multiple lengths in each convolutional block to capture temporal features at various time scales, as found to be useful in . In nutshell, ResNet takes univariate time series of any length as input and converts it to fixed-dimensional feature vector where is the number of filters in the final convolutional layer. We denote the set of all the trainable parameters of the ResNet consisting of filter weights and biases across convolutional layers, and BN layer parameters  Most ResNet implementations for TSC use feedforward layer followed softmax layer to eventually map to class probabilities, and use cross-entropy loss for training. Further, when training the ResNet for multiple tasks with varying number of classes across tasks, multi-head output with different final feedforward layer for each task is typically used, e.g. as in . However, in our setting, this implies different feedforward layer for each new few-shot task, introducing at least additional task-specific that need to be trained from scratch for each new few-shot task. This is not desirable in few-shot learning setting given only small number of samples per class, as this can lead to overfitting .g. , consider fixed number of target classes across tasks. However, we intend to learn few-shot learning algorithm that overcomes this We propose using triplet loss as the training objective which allows for generalization to varying number of classes without introducing any additional task-specific parameters, as detailed next.  Loss Function Triplet loss relies on pairwise distance between representations of time series samples from within and across classes, irrespective of the number of classes. Using triplet loss at time of fine-tuning for the test task, therefore, allows the network to adapt to given few-shot classification task without introducing any additional taskspecific parameters. Triplets consist of two matching time series and non-matching time series such that the loss aims to separate the positive pair from the negative distance margin. Given the set of all valid triplets of time series for training task of the form  , ,   consisting of an anchor time series , positive time series , and negative time series ; where the positive time series is another instance from same class as the anchor, while the negative is from different class than the anchor. We aim to obtain corresponding representations  , ,   such that the distance between the representations of an anchor and any positive time series is lower than the distance between the representations of the anchor and any negative time series. More specifically, we consider triplet loss based on Euclidean norm given by: ,  where is the distance-margin between the positive and negative pairs. The loss to be minimized is then given by . contribute to the loss. Note that since we use triplet loss for training, the number of instances per class .  FEW-SHOT LEARNING FOR UTSC TB ..  TB .. . . . . . . . . . . . .  Randomly Sampled Tasks Randomly Sampled Tasks Figure Few-Shot Training Approach. We consider meta-learning approach for few-shot UTSC based on Reptile , first-order gradient descent based meta-learning algorithm, and refer to that as . We also consider simpler variant of this approach and refer to that as similar to the training procedure of is also trained to solve multiple UTSC tasks but not explicitly trained in manner that ensures quick adaptation to any new UTSC task. Except for the triplet loss, is similar to in the way data is sampled and used for training.  Objective. learns an initialization for the parameters of the ResNet such that these parameters can be quickly optimized using gradient-based learning at test time to solve new few-shot UTSC task—i.e., the model generalizes from small number of examples from the test task. In order to learn the parameters we train the ResNet on diverse set of UTSC tasks in with varying number of classes and time series lengths. As explained in Section the same neural network parameters are shared across all tasks owing to the fact that . ResNet yields fixed-dimensional representation for varying length time series, and ii. the nature of the loss function that does not require any changes due to the varying number of classes across tasks. Similar to , we consider the following optimization problem . the learner will have low loss after updates, such that .  Implementation Details. sequentially samples fewshot tasks from the set of tasks . As summarized in Algorithm and depicted in Figure the meta-learning procedure consists of meta-iterations. Each meta-iteration involves sampling tasks. Each task, in turn, is solved using steps of gradient-based optimization, e.g. using stochastic gradient descent or Adam – this, in turn, involves randomly sampling mini-batches from the instances in the task. Each task is associated with triplet loss defined over the valid triplets as described in Section . Given that each task has varying number of instances owing to varying we set the number of iterations for each task to where is the mini-batch size and is the number of epochs. Therefore, instead of fixing the number of iterations for each sampled task, we fix the number of epochs across datasets, such that the network is trained to adapt quickly in fixed number of epochs, as described later. Also note that the number of triplets in each batch is significantly more than the number of unique time series in mini-batch. The filter weights of the ResNet are randomly initialized, e.g. via orthogonal initialization . In the meta-iteration, ResNet for each of the tasks is initialized with . Each task with labeled data is solved updating the parameters of the  network times to obtain . In practice, we use batch version of the optimization problem in Equation and use meta-batch of tasks to update as follows . Note that with implies that is updated using the updated values obtained after solving tasks for iterations each. It is this particular way of updating internally solving multiple tasks, that this algorithm is considered an example of gradient descent based meta-learning. As shown in , when performing multiple gradient updates as per Eqs. and i.e. having while solving few-shot tasks, then the expected update   is very different from taking gradient step on the expected loss   , i.e. having . In fact, it is easy to note that the update of consists of terms from the second-andhigher derivatives of due to the presence of derivatives of in Hence, the final solution using is significantly different from the one obtained using .  Fine-tuning and inference in test task. We denote the optimal parameters of ResNet after meta-training as and use this as initialization of target task-specific ResNet. For any new test task with labeled instances in and any test time series x∗ taken from first is updated to using The embeddings for all the samples in is compared to the embedding for x∗ using classifier to get the class estimate.  As shown in Algorithm is simpler variant of where instead of updating the parameters collectively using updated values from tasks, is continuously updated at each mini-batch irrespective of the task. As result, the network is trained for few iterations on task, and then the task is changed. Unlike uses only the first-order derivatives of .  EVALUATION Experimental Setup Sampling few-shot UTSC tasks. We restrict the distribution of tasks to univariate TSC with constraint on the maximum length of the time series such that . We sample tasks from the publicly available UCR Archive of UTSC datasets , where each dataset corresponds to multi-class classification task with number of classes and the length of time series varies across datasets. However, all the time series in any dataset are of same length. Each time series is using the mean and standard deviation of all the points in the time series. Out of the total of datasets on UCR Archive with we use datasets to sample tasks for training meta-set and datasets to sample tasks for the validation meta-set . Any task in or has randomly sampled time series for each of the classes in the dataset. The remaining datasets with length as listed in Table are used to create tasks for the testing meta-set. As result of this way of creating the training, validation and testing meta-sets, the classes in each meta-set are disjoint. However, the classes in the train and test sets of task in testing meta-set is, of course, the same.  Each dataset in UCR Archive is classification problem with an original train and test split. As shown in Figure we sample tasks from each of the datasets. Each task sampled from dataset contains samples from each of the classes for and samples from each of the classes for for each task are sampled from the respective original train and test split of the . The samples for each class in are sampled uniformly from the entire set of samples of the respective class. While is used for fine-tuning to get is used to evaluate the updated task-specific model Hyperparameters for and . On the basis of initial experiments on subset of the training meta-set, we use the ResNet architecture with layers and convolution filters per layer . We use Adam optimizer with learning rate of for updating on each task while using in the meta-update step in Equation . and are trained for total of meta-iterations with meta-batch size of and mini-batch size . We trained and using and for the tasks in training meta-set while is used for validation and testing meta-sets. across all experiments unless stated otherwise. We found the model with for tasks in training meta-set to be better based on average triplet loss on validation meta-set. We use epochs for solving each task while training and models. The number of epochs to be used while fine-tuning for tasks in testing metaset is chosen from the range based on average triplet loss on tasks in validation meta-set. We found and to be best for and models, respectively. Therefore, is fine-tuned for epochs for each task in testing meta-set. For the triplet loss, we use .  Baselines Considered. For comparison, we consider following baseline classifiers each using as the final classifier over raw time series or extracted ED . DTW . We use leave-one-out cross-validation on of each task to find the best warping window in the range . . . where is the window length and is the time series length. BOSS . BOSS provides symbolic representation based on Symbolic Fourier Approximation on each fixed-length sliding window extracted from time series while providing low pass filtering and quantization for noise reduction. The hyper-parameters, i.e. wordLength and normalization are chosen based on leave-one-out cross validation over the ranges and respectively, while default values of remaining hyper-parameters is used.  is applied on the extracted features for final classification decision. ResNet . The architecture is same as those used for and . Given that each task has very small number of training samples and the parameters are to be trained from scratch, ResNet architectures are likely to be prone to overfitting despite batch normalization. To mitigate this issue, apart from the same network architecture as and we also consider smaller networks with smaller number of trainable parameters. More specifically, we considered four combinations resulting from number of layers and number of filters per layer , where and . We consider the model with best overall results amongst these four combinations as baseline, viz. number of layers and number of filters . For fair comparison, each ResNet model is trained for as for . Performance Metrics. Each task is evaluated using classification accuracy rate on the test set—inference is correct if the estimated label is same as the ground truth label. Each task consists of test samples . Further, we follow the from to compare the proposed approach with various baselines considered. For each dataset, we average the classification error results over randomly sampled tasks . To study the relative performance of the approaches over multiple data sets, we compare classifiers ranks using the Friedman test and post-hoc pairwise Nemenyi test.  Results and Observations As shown in Figure we observe that improves upon all the baselines considered for tasks. The pairwise comparison of with other baselines in Figure show significant gains in accuracies across many datasets. has Win/Tie/Loss counts of when compared to the best non-few-shotlearning model, i.e. ResNet. On datasets, is amongst the models. Refer Table for dataset-wise detailed results. Our approach with simpler update rule than is the second best model but is very closely followed the ResNet models trained from scratch. To study the effect of number of training samples per class available in end task, we consider for , and experiment under same protocol of tasks . As observed ranks comparison in Table – is the best performing model, especially for and scenarios with large gaps in ranks. – When considering very small number of training samples per class, i.e. for we observe that is still the best model although it is very closely followed DTW. This is expected as given just two samples per class, it is very difficult to effectively learn any data distribution patterns, especially when the domain of the task is unseen while training. The fact that and still perform significantly better than ResNet models trained from scratch show the generic nature of filters learned in As expected, data-intensive machine learning and deep learning models like BOSS and ResNet that are trained from scratch only on the target task data tend to overfit, and are even worse than DTW. – For tasks with larger number of training samples per class, i.e. is still the best algorithm. As expected, machine learning based state-of-the-art model BOSS performs better than other baselines when sufficient training samples are available and is closer to .  To study the generalizability of to varying as result of leveraging triplet loss, we group the datasets based on As shown in Table we observe that is consistently amongst the models across values of While is significantly better than other algorithms for and it is as good as the best algorithm DTW for .  Importance of fine-tuning different layers in deep ResNet. We also study the importance of fine-tuning different convolutional layers of . We consider four variants with where we freeze parameters of lowermost convolutional layers of the pre-trained model, while fine-tuning top layers only. From Figure we observe that i.e. where the filter weights of only the first convolutional layer are frozen while those of all higher layers are fine-tuned, performs better than the default model where all layers are fine-tuned. On the other hand, freezing higher layers as well or freezing all the layers   leads to significant drop in classification performance. These results indicate that the first layer has learned generic features while being trained on diverse set of tasks and that the higher layers of the model are important to quickly adapt to the target task.  Few-shot learning to adapt to new classes for given dataset. Apart from the above scenario where the UCR datasets used to sample tasks in training, validation and testing meta-sets are different, we also consider scenario where there are large number of classes within TSC dataset, and the goal is to quickly adapt to new set of classes given model that has been pre-trained on another disjoint set of classes from the same dataset. We consider three datasets with large number of classes from the UCR Archive, namely, Adiac and ShapesAll, containing and classes, respectively. We use half of the classes to form the training meta-set, of the classes for validation meta-set, and remaining of the classes for testing meta-set. We train the and models on TSC tasks from training meta-set for and . We chose the best meta-iteration based on average triplet loss on the validation meta-set . Note that ED, DTW and BOSS are trained on the respective task from the testing meta-set only. Also, whenever number of samples for class is less than we take all samples for that class in all tasks. The average classification accuracy rates on tasks from the testing meta-set are shown in Table . We observe that outperforms other approaches indicating the ability to quickly generalize to new classes for given domain.  Non-few-shot learning scenario. We also evaluate when sufficient labeled data is available for training, i.e. the standard non-few-shot learning scenario with original class distributions and train-test splits as provided in . As shown in Figure we observe that the meta-learned outperforms other approaches even in non-few-shot scenarios proving the benefit of meta-learning based initialization. Furthermore, when compared to the results in Figure we observe increased performance gap between the deep learning approaches and other approaches due to availability of sufficient training data. We provide scatter-plot comparison for with second best approach ResNet in Figure and omit other dataset-wise results for lack of space.  CONCLUSION AND FUTURE WORK The ability to quickly adapt to any given time series classification task with small number of labeled samples is an important task with several practical applications. We have proposed metalearning approach for few-shot time series classification . It can also be seen as data-efficient metric learning mechanism that leverages pre-trained model. We have shown that it is possible to train model on few-shot tasks from diverse domains such that the model gathers an ability to quickly generalize and solve few-shot tasks from previously unseen domains. leveraging the triplet loss, we are able to generalize across classification tasks with different number of classes. We hope that this work opens promising direction for future research in meta-learning for time series modeling. In this work, we have explored first-order meta-learning algorithms. In future, it would be interesting to explore more sophisticated meta-learning algorithms such as for the same. similar approach for time series forecasting will be interesting to explore as well.